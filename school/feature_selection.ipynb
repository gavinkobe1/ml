{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIXDYUwuOKsn"
   },
   "source": [
    "<h1 style=\"text-align: center;\">Lab 4: Variable Importance & Feature Selection</h1>\n",
    "\n",
    "**At the end of this practical you should be able to:**\n",
    "1. Compute variable importance measures using both *filter* and *permutation importance* measures and understand the difference\n",
    "2. Perform and understand the difference between different feature selection methods (minimally optimal)\n",
    "\n",
    "The data set for the practical is an **Online News Popularity Data Set** where the task is to predict how many shares an article will get as a binary value (\"low\" or \"high\").\n",
    "\n",
    "## This practical is in two parts. The first part will be done as a demonstration. The second part is for you to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdXeNxfmcRDd"
   },
   "source": [
    "# Demo\n",
    "## Demo Part 1: Creating a function to compare and print our variable importances\n",
    "Since we want to compare different methods for computing variable importance we are going to want to print them side-by-side ordered by ranking.\n",
    "\n",
    "While we could write the code each time, it gets a bit repetitive. So we'll write a function that takes a dictionary of feature_importance scores and the list of feature names and prints the table.\n",
    "\n",
    "*Why a dictionary?* As each list of feature importance scores needs a name, so key = name of method, value = feature importance scores.\n",
    "\n",
    "**You do not have to understand this function for this practical** - just what it does and how to call it. I.e. read the documentation at the top of the function. That said, you should be able to understand it, it only contains python you know/know how to look up. If you've time once you've finished this practical look back of the function and try and understand it. Ask if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iFt5h7bNcXMG"
   },
   "outputs": [],
   "source": [
    "def print_variable_importances( feature_names, dict_in, show_top = 10 ):\n",
    "  \"\"\"\n",
    "  Prints a table of feature importance scores.\n",
    "  \n",
    "  Keyword arguments\n",
    "  feature_names -- list of feature names. Must have the same ordering as the \n",
    "                   scores in each instance of list_of_scores (see below)\n",
    "  dict_in       -- dictionary of the form {method_name:list_of_scores} where:\n",
    "                   method_name    -- string\n",
    "                   list_of_scores -- list of scores, ordered in the same order\n",
    "                                     as the passed feature_names\n",
    "  show_top      -- number of features to show (default 10, None to print all)\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Implement the definition of None for show_top\n",
    "  if show_top is None:\n",
    "    show_top = len(feature_names)\n",
    "  \n",
    "  # Set up lists to hold the titles and score_feature tuples\n",
    "  # We need a list so that they maintain fixed order as we \n",
    "  # iterate over them row-by-row.\n",
    "  to_print_titles = []\n",
    "  to_print_scores = []\n",
    "  \n",
    "  # Pair each list of scores with a copy of the feature names and sort\n",
    "  # based on the variable importance score descending\n",
    "  for k, v in dict_in.items():\n",
    "    # zip pairs, sorted sorts, reverse orders descending\n",
    "    feature_names_plus_scores = sorted( zip(v, feature_names) )\n",
    "    feature_names_plus_scores.reverse()\n",
    "    to_print_titles.append(k)\n",
    "    to_print_scores.append(feature_names_plus_scores)\n",
    "    \n",
    "    \n",
    "  # Print the scores\n",
    "  \n",
    "  # Create a list of strings to print in each header cell\n",
    "  line_parts = []\n",
    "  for j in range(len(to_print_titles)):\n",
    "    line_parts.append('{:<38}'.format(to_print_titles[j]))\n",
    "  \n",
    "  # Print each header cell using a separator ' | ', adding the fixed rank column header \n",
    "  print('Rank | ' + ' | '.join( ['{:<38}'.format(x) for x in to_print_titles] ) )\n",
    "  \n",
    "  # Print the header underline\n",
    "  print('---- + ' + ' + '.join( [ '-'*38 ]*len(to_print_titles) ) )\n",
    "  \n",
    "  # Print each line\n",
    "  for i in range(show_top):\n",
    "    # Create a list of strings to print for each row cell\n",
    "    line_parts = []\n",
    "    for j in range(len(to_print_titles)):\n",
    "      line_parts.append(  '{:<30}: {:.4f}'.format(to_print_scores[j][i][1], to_print_scores[j][i][0]) )\n",
    "    # Print the row by:\n",
    "    # (1) joining each row cell using a separator ' | '\n",
    "    # (2) adding the fixed rank column header \n",
    "    print( '{:<4} | '.format(str(i)) + ' | '.join(line_parts) )\n",
    "      \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWlhFw_ycBRI"
   },
   "source": [
    "## Demo Part 2: Data loading and create our training and test splits\n",
    "You should be familar with this by now. The data set we will use is has been uploaded to\n",
    "<br>http://www.cs.nott.ac.uk/~pszgss/teaching/OnlineNewsPopularity.csv <br>\n",
    "Full details about the dataset can be found at:\n",
    "<br>https://archive.ics.uci.edu/ml/datasets/online+news+popularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YSee73FGOKss"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Check the column names (i.e. print them). What is wrong? Yes, you will need to fix that.\n",
    "# HINT: Strings have a .strip() method which returns the string without any leading or training whitespace.\n",
    "#       I'd advise creating a new (corrected) array of column headings and update data.columns\n",
    "\n",
    "from pandas import read_csv\n",
    "data = read_csv('http://www.cs.nott.ac.uk/~pszgss/teaching/OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "id": "1BeJgOBJbt7-",
    "outputId": "b43a8e50-0715-4eb5-9173-2f429bc21b31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...       731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...       731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...       731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "1              9.0              255.0          0.604743                1.0   \n",
       "2              9.0              211.0          0.575130                1.0   \n",
       "3              9.0              531.0          0.503788                1.0   \n",
       "4             13.0             1072.0          0.415646                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  ...  \\\n",
       "0                   0.815385         4.0              2.0        1.0  ...   \n",
       "1                   0.791946         3.0              1.0        1.0  ...   \n",
       "2                   0.663866         3.0              1.0        1.0  ...   \n",
       "3                   0.665635         9.0              0.0        1.0  ...   \n",
       "4                   0.540890        19.0             19.0       20.0  ...   \n",
       "\n",
       "    min_positive_polarity   max_positive_polarity   avg_negative_polarity  \\\n",
       "0                0.100000                     0.7               -0.350000   \n",
       "1                0.033333                     0.7               -0.118750   \n",
       "2                0.100000                     1.0               -0.466667   \n",
       "3                0.136364                     0.8               -0.369697   \n",
       "4                0.033333                     1.0               -0.220192   \n",
       "\n",
       "    min_negative_polarity   max_negative_polarity   title_subjectivity  \\\n",
       "0                  -0.600               -0.200000             0.500000   \n",
       "1                  -0.125               -0.100000             0.000000   \n",
       "2                  -0.800               -0.133333             0.000000   \n",
       "3                  -0.600               -0.166667             0.000000   \n",
       "4                  -0.500               -0.050000             0.454545   \n",
       "\n",
       "    title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  -0.187500                 0.000000   \n",
       "1                   0.000000                 0.500000   \n",
       "2                   0.000000                 0.500000   \n",
       "3                   0.000000                 0.500000   \n",
       "4                   0.136364                 0.045455   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                       0.187500      593  \n",
       "1                       0.000000      711  \n",
       "2                       0.000000     1500  \n",
       "3                       0.000000     1200  \n",
       "4                       0.136364      505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qh7laJ7XceEI",
    "outputId": "ff8ac5d0-1732-40cf-a246-e4644a53d80a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
       "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
       "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
       "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
       "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
       "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
       "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
       "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
       "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
       "       ' self_reference_max_shares', ' self_reference_avg_sharess',\n",
       "       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n",
       "       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n",
       "       ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',\n",
       "       ' LDA_03', ' LDA_04', ' global_subjectivity',\n",
       "       ' global_sentiment_polarity', ' global_rate_positive_words',\n",
       "       ' global_rate_negative_words', ' rate_positive_words',\n",
       "       ' rate_negative_words', ' avg_positive_polarity',\n",
       "       ' min_positive_polarity', ' max_positive_polarity',\n",
       "       ' avg_negative_polarity', ' min_negative_polarity',\n",
       "       ' max_negative_polarity', ' title_subjectivity',\n",
       "       ' title_sentiment_polarity', ' abs_title_subjectivity',\n",
       "       ' abs_title_sentiment_polarity', ' shares'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TxAd9iCsiuyy"
   },
   "outputs": [],
   "source": [
    "new_columns = []\n",
    "for col in data.columns:\n",
    "  new_columns.append( col.strip() )\n",
    "\n",
    "data.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZFdFyI5fjSx",
    "outputId": "742f74b8-2137-4a33-8ef9-a3db19092439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39644"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSEODhZHjK9T",
    "outputId": "2c7837b5-db07-4537-c9bc-5c1fa94cd1d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         593\n",
       "1         711\n",
       "2        1500\n",
       "3        1200\n",
       "4         505\n",
       "         ... \n",
       "39639    1800\n",
       "39640    1900\n",
       "39641    1900\n",
       "39642    1100\n",
       "39643    1300\n",
       "Name: shares, Length: 39644, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmuLZqNigoGH"
   },
   "source": [
    "We now need to select the input and output features.<br>\n",
    "**HINT:** Not all input features should be included.\n",
    "\n",
    "The output feature currently is continuous. Recall that, for this task, we want to predict how many shares an article will get as a binary value (\"low\" or \"high\"). \n",
    "\n",
    "* High is defined as shares >= 1400\n",
    "* Low is defined as shares < 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zvt6Nw5YfewS"
   },
   "outputs": [],
   "source": [
    "# Select the input and output features\n",
    "# url and timedelta are considered non-predictive in the documentation and \n",
    "# therefore should not be used as an input feature\n",
    "X = data.drop(columns = ['shares', 'timedelta', 'url'])\n",
    "y = data.shares\n",
    "y.values[y < 1400] = 0\n",
    "y.values[y>=1400] =  1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUoJwTUhlJqI"
   },
   "source": [
    "Finally we need to split our data into test and training sets.\n",
    "\n",
    "We will perform all feature selection based on the training set ONLY. \n",
    "\n",
    "The test set will be used to test if our feature selection has helped or hurt.\n",
    "\n",
    "Use a `test_size=0.33` and a `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "W8DmwoD1lPle"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mGaAAGIcTKV"
   },
   "source": [
    "## Demo Part 3: Removing features with low variance\n",
    "The first thing to do is to check to see if any features (nearly) always have the same value.\n",
    "\n",
    "This is done via a [`VarianceThreshold`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) object. The `VarianceThreshold` Object takes one parameter:\n",
    "* threshold -- the variance threshold below which to remove features\n",
    "\n",
    "`VarianceThreshold` follows the **fit transform** interface and can be used as a pre-processing step within a pipeline.\n",
    "\n",
    "**`.fit(...)`:** Learns which features have low variance.<br>\n",
    "**`.transform(...)`:** Returns a copy of the data removing features with variance below the threshold. \n",
    "\n",
    "After calling `.fit(..)` you can use the:<br>\n",
    "`.get_support()` method to get a boolean mask of which features was selected.\n",
    "\n",
    "Assuming your VarianceThreshold object is called vt, then you can see the list of features that were selected via:<br>\n",
    "`X_train.columns[vt.get_support()]`\n",
    "<br>If you do not understand what that line of code is doing, please ask.\n",
    "\n",
    "**TASKS:**\n",
    "1. Using the methods detailed above write the code to print the selected features with a threshold of zero.\n",
    "2. The function `numpy.invert(...)` will invert a binary mask. Write the code to print the removed features rather than those kept.\n",
    "3. Are any features removed?\n",
    "4. What happens if you change the threshold? \n",
    "5. What would a good value be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "5HDca2ORdU8A",
    "outputId": "af96fa7e-1e2a-473d-9c60-2edac3721293"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.398749</td>\n",
       "      <td>546.514731</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>0.996469</td>\n",
       "      <td>0.689175</td>\n",
       "      <td>10.883690</td>\n",
       "      <td>3.293638</td>\n",
       "      <td>4.544143</td>\n",
       "      <td>1.249874</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.114037</td>\n",
       "      <td>471.107508</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>5.231231</td>\n",
       "      <td>3.264816</td>\n",
       "      <td>11.332017</td>\n",
       "      <td>3.855141</td>\n",
       "      <td>8.309434</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625739</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>409.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>716.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.754630</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>1042.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "count    39644.000000      39644.000000     39644.000000      39644.000000   \n",
       "mean        10.398749        546.514731         0.548216          0.996469   \n",
       "std          2.114037        471.107508         3.520708          5.231231   \n",
       "min          2.000000          0.000000         0.000000          0.000000   \n",
       "25%          9.000000        246.000000         0.470870          1.000000   \n",
       "50%         10.000000        409.000000         0.539226          1.000000   \n",
       "75%         12.000000        716.000000         0.608696          1.000000   \n",
       "max         23.000000       8474.000000       701.000000       1042.000000   \n",
       "\n",
       "       n_non_stop_unique_tokens     num_hrefs  num_self_hrefs      num_imgs  \\\n",
       "count              39644.000000  39644.000000    39644.000000  39644.000000   \n",
       "mean                   0.689175     10.883690        3.293638      4.544143   \n",
       "std                    3.264816     11.332017        3.855141      8.309434   \n",
       "min                    0.000000      0.000000        0.000000      0.000000   \n",
       "25%                    0.625739      4.000000        1.000000      1.000000   \n",
       "50%                    0.690476      8.000000        3.000000      1.000000   \n",
       "75%                    0.754630     14.000000        4.000000      4.000000   \n",
       "max                  650.000000    304.000000      116.000000    128.000000   \n",
       "\n",
       "         num_videos  average_token_length  ...  avg_positive_polarity  \\\n",
       "count  39644.000000          39644.000000  ...           39644.000000   \n",
       "mean       1.249874              4.548239  ...               0.353825   \n",
       "std        4.107855              0.844406  ...               0.104542   \n",
       "min        0.000000              0.000000  ...               0.000000   \n",
       "25%        0.000000              4.478404  ...               0.306244   \n",
       "50%        0.000000              4.664082  ...               0.358755   \n",
       "75%        1.000000              4.854839  ...               0.411428   \n",
       "max       91.000000              8.041534  ...               1.000000   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.095446               0.756728              -0.259524   \n",
       "std                 0.071315               0.247786               0.127726   \n",
       "min                 0.000000               0.000000              -1.000000   \n",
       "25%                 0.050000               0.600000              -0.328383   \n",
       "50%                 0.100000               0.800000              -0.253333   \n",
       "75%                 0.100000               1.000000              -0.186905   \n",
       "max                 1.000000               1.000000               0.000000   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "count           39644.000000           39644.000000        39644.000000   \n",
       "mean               -0.521944              -0.107500            0.282353   \n",
       "std                 0.290290               0.095373            0.324247   \n",
       "min                -1.000000              -1.000000            0.000000   \n",
       "25%                -0.700000              -0.125000            0.000000   \n",
       "50%                -0.500000              -0.100000            0.150000   \n",
       "75%                -0.300000              -0.050000            0.500000   \n",
       "max                 0.000000               0.000000            1.000000   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count              39644.000000            39644.000000   \n",
       "mean                   0.071425                0.341843   \n",
       "std                    0.265450                0.188791   \n",
       "min                   -1.000000                0.000000   \n",
       "25%                    0.000000                0.166667   \n",
       "50%                    0.000000                0.500000   \n",
       "75%                    0.150000                0.500000   \n",
       "max                    1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity  \n",
       "count                  39644.000000  \n",
       "mean                       0.156064  \n",
       "std                        0.226294  \n",
       "min                        0.000000  \n",
       "25%                        0.000000  \n",
       "50%                        0.000000  \n",
       "75%                        0.250000  \n",
       "max                        1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uuWDcSF4hCp1",
    "outputId": "89128a96-21f6-4435-8599-816a0b801e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
      "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs',\n",
      "       'num_self_hrefs', 'num_imgs', 'num_videos', 'average_token_length',\n",
      "       'num_keywords', 'data_channel_is_lifestyle',\n",
      "       'data_channel_is_entertainment', 'data_channel_is_bus',\n",
      "       'data_channel_is_socmed', 'data_channel_is_tech',\n",
      "       'data_channel_is_world', 'kw_min_min', 'kw_max_min', 'kw_avg_min',\n",
      "       'kw_min_max', 'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg',\n",
      "       'kw_avg_avg', 'self_reference_min_shares', 'self_reference_max_shares',\n",
      "       'self_reference_avg_sharess', 'weekday_is_monday', 'weekday_is_tuesday',\n",
      "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
      "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend', 'LDA_00',\n",
      "       'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
      "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
      "       'global_rate_negative_words', 'rate_positive_words',\n",
      "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
      "       'max_positive_polarity', 'avg_negative_polarity',\n",
      "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
      "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
      "       'abs_title_sentiment_polarity'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TASK 1\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=0.0) # 将方差小于等于1.0的特征删除\n",
    "\n",
    "vt.fit(X)\n",
    "print(X.columns[vt.get_support()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "XqdqTFQ9U_d_",
    "outputId": "9f0d062a-7964-4bc7-a08e-4ccd33781399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TASK 2\n",
    "vt.fit(X)\n",
    "print(X.columns[np.invert(vt.get_support())])\n",
    "\n",
    "# TASK 3: Answer: \n",
    "# no, because the threshold is 0.0\n",
    "\n",
    "# TASK 4: Answer:\n",
    "# If we increase the value of threshold gradually, the number of the selected features decreases and the number of the removed features increases.\n",
    "\n",
    "# TASK 5: Answer:\n",
    "# First, we increase the value of threshold gradually, and when the number of selected variables keep  unchanged, the threshold would be a good value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f0FJwuYlVoJ"
   },
   "source": [
    "## Demo Part 4: Univariate variable importance  (known as filter based methods when used for selection)\n",
    "Features can be ranked by (typically simple) measures of how they **individually** affect the output variable. There are many measures we can use, [a number of them are implemented in sklearn](http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection).\n",
    "\n",
    "Univariate feature ranking and selection (all measures) are all implemented in sklearn via the [`GenericUnivariateSelect`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html) object. \n",
    "\n",
    "`GenericUnivariateSelect` follows the **fit transform** interface and can be used as a pre-processing step within a pipeline.\n",
    "\n",
    "**`GenericUnivariateSelect(...)`:** Constructor. Takes parameters which specify which univariate feature importance measure will be calculated (when calling `.fit(...)`) and the strategy to select the \"best\" features for creating a reduced feature dataset (when calling `.transform(...)`). See the documentation for the list of avaliable univariate scoring functions.\n",
    "\n",
    "**`.fit(...)`:** Computes variable importance scores. After calling `.fit(...)` the ranking is avaliable within the object as the attribute `.scores_`<br>\n",
    "\n",
    "**`.transform(...)`:** Takes the input features and creates a new input features dataset (transforms the original dataset into the new dataset) by selecting a subset of the features as defined by the paramters `mode` and `param`. For instance, if the following parameters were passed when creating the GenericUnivariateSelect object (via the cons)  `mode = 'k_best'` and `param = 10`, `.transform(...)` will return a dataset with only the top 10 input features as ranked.\n",
    "\n",
    "If we only want to rank we still set up a selector and call `.fit(..)` to compute the rankings, but simply do not end up selecting features based on it (via `.transform(...)`).\n",
    "\n",
    "After calling `.fit(...)` the scores are available as the attribute: `.scores_`\n",
    "\n",
    "**TASK:** \n",
    "1. First ranking the features via the scoring function: `mutual_info_classif` \n",
    "2. Create a new dataset `X_train_b` by selecting only the top 10 features. \n",
    "3. Unrelated to (1) & (2) create a Pipeline that incorporates kBest features selection via `to mutual_info_classif` where k = 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "muATkYYAoJ9n",
    "outputId": "c8ea8314-8e54-406e-8126-7889fee028ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericUnivariateSelect(mode='k_best', param=10,\n",
       "                        score_func=<function mutual_info_classif at 0x7fd0ea44d040>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import mutual_info_classif  \n",
    "#Estimate mutual information for a discrete target variable. (mutual_info_regression)\n",
    "#Mutual information (MI) [1] between two random variables is a non-negative value, \n",
    "#which measures the dependency between the variables. It is equal to zero if \n",
    "#and only if two random variables are independent, and higher values mean higher dependency.\n",
    "us = GenericUnivariateSelect( score_func=mutual_info_classif, mode='k_best', param=10 ) # removes all but the k highest scoring features\n",
    "\n",
    "us.fit(X,y)F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "rO_LSuqzm45o",
    "outputId": "9422b530-3e5e-4956-c015-439086a99e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank | Filter                                \n",
      "---- + --------------------------------------\n",
      "0    | kw_max_avg                    : 0.0354\n",
      "1    | LDA_02                        : 0.0349\n",
      "2    | self_reference_max_shares     : 0.0275\n",
      "3    | LDA_03                        : 0.0260\n",
      "4    | LDA_00                        : 0.0258\n",
      "5    | self_reference_avg_sharess    : 0.0255\n",
      "6    | LDA_04                        : 0.0250\n",
      "7    | self_reference_min_shares     : 0.0236\n",
      "8    | LDA_01                        : 0.0234\n",
      "9    | kw_avg_avg                    : 0.0224\n"
     ]
    }
   ],
   "source": [
    "feature_importance_scores = {}\n",
    "feature_importance_scores['Filter'] = us.scores_\n",
    "\n",
    "print_variable_importances( X_train.columns, feature_importance_scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "GrcyuJKHnP9x",
    "outputId": "ea8bfa8c-d81c-4f16-b21d-facb87971ab6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.050001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.682188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1      2      3      4         5         6         7         8  \\\n",
       "0  0.0  0.0  496.0  496.0  496.0  0.500331  0.378279  0.040005  0.041263   \n",
       "1  0.0  0.0    0.0    0.0    0.0  0.799756  0.050047  0.050096  0.050101   \n",
       "2  0.0  0.0  918.0  918.0  918.0  0.217792  0.033334  0.033351  0.033334   \n",
       "\n",
       "          9  \n",
       "0  0.040123  \n",
       "1  0.050001  \n",
       "2  0.682188  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_new = pd.DataFrame(us.transform(X))\n",
    "X_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "498hIBSgXGex",
    "outputId": "a0090329-336c-4771-b444-94a4ec4e8dbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00241314, 0.00106773, 0.        , 0.00074268, 0.00142929,\n",
       "       0.00433493, 0.003496  , 0.00925216, 0.00277746, 0.0006124 ,\n",
       "       0.        , 0.        , 0.0090751 , 0.00197782, 0.00586922,\n",
       "       0.00524239, 0.00642788, 0.00404207, 0.01851066, 0.01108851,\n",
       "       0.01595585, 0.00311623, 0.0145863 , 0.0173789 , 0.03523229,\n",
       "       0.02252099, 0.0234771 , 0.02065639, 0.02472744, 0.        ,\n",
       "       0.00373516, 0.00305178, 0.00221219, 0.        , 0.00874051,\n",
       "       0.00402403, 0.01044491, 0.02535043, 0.02241863, 0.03448697,\n",
       "       0.0256491 , 0.02484497, 0.00354284, 0.00423748, 0.00692306,\n",
       "       0.        , 0.00685123, 0.00917724, 0.00451724, 0.00053008,\n",
       "       0.        , 0.00105561, 0.00313059, 0.0001305 , 0.00110673,\n",
       "       0.00633661, 0.        , 0.00067176])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo Task 4.1: First create the GenericUnivariateSelect object, then fit it to compute the scores\n",
    "\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "us = GenericUnivariateSelect( score_func=mutual_info_classif, mode='k_best', param=10 )\n",
    "\n",
    "us.fit(X,y)\n",
    "us.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_dt022-QiETM"
   },
   "outputs": [],
   "source": [
    "# Demo Task 4.2: Create a new dataset X_train_b by selecting only the top 10 features.\n",
    "\n",
    "X_train_b = us.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bqMsp8zSjmMr",
    "outputId": "c821d80f-16ec-4b73-cef2-9ed8b7425ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5880149812734082\n"
     ]
    }
   ],
   "source": [
    "# Demo Task 4.3: Unrelated to (1) & (2) create a Pipeline that incorporates kBest features selection via to mutual_info_classif where k = 10. \n",
    "# Terminate the Pipeline with a KNeighborsClassifier\n",
    "# Evaluate the performance of the classifier. Think about other pre-processing steps that might also be useful and include them.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "steps = [('select',StandardScaler()),\n",
    "         ('ss', GenericUnivariateSelect( score_func = mutual_info_classif, mode = 'k_best', param = 10)),\n",
    "         ('model', KNeighborsClassifier())]\n",
    "\n",
    "knn = Pipeline( steps = steps ) \n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "scores = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hdb3wYGSm9yi"
   },
   "outputs": [],
   "source": [
    "# Of course 10 may or may not be the best number to pick. If you feel like it look\n",
    "# back at the scores and try another value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLFUK_I-iqw0"
   },
   "source": [
    "## Demo Part 5: Model Based Methods\n",
    "Rather than using external measures of how features **individually** affect the output feature one can rank features by how much they are used in a trained model. An example of this is Random Forests.\n",
    "\n",
    "In sklearn, Random Forests have an attribute `feature_importances_` which stores the scores each feature as a list denoting their importance. These scores are computed when the model is fit.\n",
    "\n",
    "**TASK:**\n",
    "Train a RandomForestClassifier and print out a list of feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvCuSAcxU_MX",
    "outputId": "74c2aa71-33dd-4446-93fa-8b098c69f04d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank | Filter                                 | Embeded RF                            \n",
      "---- + -------------------------------------- + --------------------------------------\n",
      "0    | kw_max_avg                    : 0.0354 | kw_avg_avg                    : 0.0437\n",
      "1    | LDA_02                        : 0.0349 | kw_max_avg                    : 0.0403\n",
      "2    | self_reference_max_shares     : 0.0275 | LDA_02                        : 0.0328\n",
      "3    | LDA_03                        : 0.0260 | self_reference_min_shares     : 0.0321\n",
      "4    | LDA_00                        : 0.0258 | kw_avg_max                    : 0.0302\n",
      "5    | self_reference_avg_sharess    : 0.0255 | LDA_01                        : 0.0298\n",
      "6    | LDA_04                        : 0.0250 | kw_avg_min                    : 0.0295\n",
      "7    | self_reference_min_shares     : 0.0236 | LDA_04                        : 0.0294\n",
      "8    | LDA_01                        : 0.0234 | self_reference_avg_sharess    : 0.0284\n",
      "9    | kw_avg_avg                    : 0.0224 | LDA_00                        : 0.0280\n",
      "10   | kw_min_avg                    : 0.0192 | global_subjectivity           : 0.0277\n",
      "11   | kw_max_min                    : 0.0156 | n_non_stop_unique_tokens      : 0.0275\n",
      "12   | data_channel_is_world         : 0.0156 | average_token_length          : 0.0274\n",
      "13   | kw_avg_max                    : 0.0145 | n_unique_tokens               : 0.0270\n",
      "14   | kw_min_max                    : 0.0128 | n_tokens_content              : 0.0258\n",
      "15   | kw_avg_min                    : 0.0113 | LDA_03                        : 0.0258\n",
      "16   | num_keywords                  : 0.0104 | global_rate_positive_words    : 0.0257\n",
      "17   | rate_positive_words           : 0.0093 | kw_max_min                    : 0.0253\n",
      "18   | num_imgs                      : 0.0086 | avg_positive_polarity         : 0.0251\n",
      "19   | rate_negative_words           : 0.0084 | global_sentiment_polarity     : 0.0249\n",
      "20   | data_channel_is_tech          : 0.0084 | avg_negative_polarity         : 0.0241\n",
      "21   | kw_max_max                    : 0.0081 | self_reference_max_shares     : 0.0233\n",
      "22   | title_sentiment_polarity      : 0.0076 | global_rate_negative_words    : 0.0232\n",
      "23   | weekday_is_sunday             : 0.0075 | num_hrefs                     : 0.0221\n",
      "24   | weekday_is_saturday           : 0.0068 | kw_min_avg                    : 0.0218\n",
      "25   | is_weekend                    : 0.0068 | rate_negative_words           : 0.0206\n",
      "26   | global_rate_positive_words    : 0.0065 | rate_positive_words           : 0.0206\n",
      "27   | data_channel_is_entertainment : 0.0055 | kw_min_max                    : 0.0167\n",
      "28   | num_videos                    : 0.0054 | n_tokens_title                : 0.0162\n",
      "29   | global_sentiment_polarity     : 0.0052 | title_sentiment_polarity      : 0.0150\n",
      "30   | min_negative_polarity         : 0.0046 | num_imgs                      : 0.0146\n",
      "31   | data_channel_is_socmed        : 0.0043 | min_negative_polarity         : 0.0144\n",
      "32   | avg_positive_polarity         : 0.0042 | max_negative_polarity         : 0.0139\n",
      "33   | global_rate_negative_words    : 0.0040 | title_subjectivity            : 0.0131\n",
      "34   | min_positive_polarity         : 0.0036 | num_self_hrefs                : 0.0129\n",
      "35   | kw_min_min                    : 0.0036 | abs_title_subjectivity        : 0.0123\n",
      "36   | global_subjectivity           : 0.0026 | min_positive_polarity         : 0.0123\n",
      "37   | n_tokens_content              : 0.0026 | abs_title_sentiment_polarity  : 0.0118\n",
      "38   | max_positive_polarity         : 0.0025 | num_keywords                  : 0.0105\n",
      "39   | title_subjectivity            : 0.0023 | max_positive_polarity         : 0.0103\n",
      "40   | abs_title_subjectivity        : 0.0023 | is_weekend                    : 0.0099\n",
      "41   | max_negative_polarity         : 0.0019 | data_channel_is_entertainment : 0.0092\n",
      "42   | weekday_is_tuesday            : 0.0018 | num_videos                    : 0.0080\n",
      "43   | n_non_stop_words              : 0.0014 | kw_max_max                    : 0.0069\n",
      "44   | average_token_length          : 0.0013 | data_channel_is_tech          : 0.0058\n",
      "45   | num_hrefs                     : 0.0013 | kw_min_min                    : 0.0057\n",
      "46   | data_channel_is_bus           : 0.0010 | data_channel_is_socmed        : 0.0055\n",
      "47   | n_non_stop_unique_tokens      : 0.0009 | data_channel_is_world         : 0.0054\n",
      "48   | weekday_is_friday             : 0.0008 | weekday_is_saturday           : 0.0047\n",
      "49   | weekday_is_monday             : 0.0005 | weekday_is_wednesday          : 0.0035\n",
      "50   | n_tokens_title                : 0.0004 | weekday_is_tuesday            : 0.0033\n",
      "51   | abs_title_sentiment_polarity  : 0.0004 | weekday_is_friday             : 0.0031\n",
      "52   | num_self_hrefs                : 0.0002 | weekday_is_thursday           : 0.0030\n",
      "53   | weekday_is_wednesday          : 0.0000 | weekday_is_monday             : 0.0030\n",
      "54   | weekday_is_thursday           : 0.0000 | weekday_is_sunday             : 0.0027\n",
      "55   | n_unique_tokens               : 0.0000 | data_channel_is_bus           : 0.0022\n",
      "56   | data_channel_is_lifestyle     : 0.0000 | data_channel_is_lifestyle     : 0.0016\n",
      "57   | avg_negative_polarity         : 0.0000 | n_non_stop_words              : 0.0001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "feature_importance_scores['Embeded RF'] = rf.feature_importances_\n",
    "\n",
    "print_variable_importances( X_train.columns, feature_importance_scores, show_top=None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAgwDC-Hsy90"
   },
   "source": [
    "## Part 6: Model Based Methods: Permutation Importance\n",
    "Looking at the feature importance scores from the random forest it is hard to know if we should keep the features or not. Moreover, if we wanted to consider feature importance from, say a non-linear SVM, we couldn't. Therefore we will now consider *permutation importance*. \n",
    "\n",
    "<br>To do this we will use the [`sklearn.inspection.permutation_importance` package](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance). \n",
    "\n",
    "**NOTE: We want our mean decrease in accuracy to be with respect to generalization accuracy (i.e. on an independent test set, not the training set)**. Therefore we need to compute the permutation scores based on both a training and (independent) test set **or** repeatedly break our data into training and test sets and evaluate via a cross-validation procedure. Note the \"data\" here to split (either manually or via a cross-validation procedure) is our current training data. The current test data is only for testing our prediction accuracy after we have removed features.\n",
    "\n",
    "### In this version, we are going to create a separate training and test set (from our current train data) for evaluating the MDA.\n",
    "This is simpliest way to get a permutation importance and involves **wrapping a fitted model** in a `permutation_importance` object. The signiture of the `permutation_importance` constructor is:\n",
    "`sklearn.inspection.permutation_importance(estimator, X, y, *, scoring=None, n_repeats=5, n_jobs=None, random_state=None, sample_weight=None, max_samples=1.0)`\n",
    "<br>\n",
    "The permutation importance of a feature is calculated as follows. First, a baseline metric, defined by scoring, is evaluated on a (potentially different) dataset defined by the X. Next, a feature column from the validation set is permuted and the metric is evaluated again. The permutation importance is defined to be the difference between the baseline metric and metric from permutating the feature column.\n",
    "<br>**TASK:**\n",
    "1. Wrap a trained (on X_train, y_train) RandomForest model in a `permutation_importance` object to generate the feature importances. You should pass the X_train and y_train data, with `n_repeats=2, random_state=0`.  \n",
    "2. Add the feature importances to the results dictionary and print the dictionary to compare the feature importances to those from the previous exercise. These can be accessed from the `.importances_mean` attribute in the `permutation_importance` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "35p7hU809-Nz"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TASK 1 \n",
    "rf_perm = RandomForestClassifier()\n",
    "rf_perm.fit(X_train, y_train)\n",
    "perm = permutation_importance(rf_perm,X_train, y_train,n_repeats=2, random_state=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QF6ne9mhN5vO",
    "outputId": "510bad64-6bef-4bf8-9d8c-009390b1a3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank | Filter                                 | Embeded RF                             | Perm RF (58.80)                       \n",
      "---- + -------------------------------------- + -------------------------------------- + --------------------------------------\n",
      "0    | kw_max_avg                    : 0.0354 | kw_avg_avg                    : 0.0437 | kw_avg_avg                    : 0.0135\n",
      "1    | LDA_02                        : 0.0349 | kw_max_avg                    : 0.0403 | kw_max_avg                    : 0.0117\n",
      "2    | self_reference_max_shares     : 0.0275 | LDA_02                        : 0.0328 | is_weekend                    : 0.0117\n",
      "3    | LDA_03                        : 0.0260 | self_reference_min_shares     : 0.0321 | self_reference_min_shares     : 0.0104\n",
      "4    | LDA_00                        : 0.0258 | kw_avg_max                    : 0.0302 | kw_min_avg                    : 0.0064\n",
      "5    | self_reference_avg_sharess    : 0.0255 | LDA_01                        : 0.0298 | self_reference_avg_sharess    : 0.0059\n",
      "6    | LDA_04                        : 0.0250 | kw_avg_min                    : 0.0295 | data_channel_is_entertainment : 0.0036\n",
      "7    | self_reference_min_shares     : 0.0236 | LDA_04                        : 0.0294 | LDA_02                        : 0.0029\n",
      "8    | LDA_01                        : 0.0234 | self_reference_avg_sharess    : 0.0284 | LDA_01                        : 0.0017\n",
      "9    | kw_avg_avg                    : 0.0224 | LDA_00                        : 0.0280 | kw_avg_min                    : 0.0013\n"
     ]
    }
   ],
   "source": [
    "# TASK 2\n",
    "method_name = 'Perm RF ({:.2f})'.format(scores*100)\n",
    "\n",
    "feature_importance_scores[method_name] = perm.importances_mean\n",
    "\n",
    "print_variable_importances(X_train.columns, feature_importance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5uJjwyjlcKt"
   },
   "source": [
    "Looking at the results what do you notice? Specifically consider the permutation importance scores remembering their interpretation. Try summing the features, what do you notice? \n",
    "What can we infer from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ab0lkQuKm42s"
   },
   "outputs": [],
   "source": [
    "# The prediction score using both two methods are similar.\n",
    "# Eight of ten features are included in both two methods.\n",
    "# These eight features should be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxnrruaRuH6k"
   },
   "source": [
    "## Part 7: Wrapper Methods\n",
    "We know that considering the effect of holding out variables and/or their relative use within a given model does not show how *predictive* they actually are. This is because other variables may be *masking* and other interaction effects going on.<br>\n",
    "\n",
    "**Example:** If predicting someones country of birth the feature `country_of_drivers_licence` may not be listed as that predictive as it should have been if the feature `passport_country` exists since the model could have used either in a model dependent way. In random forests they are used equally meaning the predictive amount will be half of what it should have been on average in this case).\n",
    "\n",
    "Wrapper methods perform feature subset search to try and find an optimal subset of features. Typically this is done for **feature selection** rather than **feature understanding**. \n",
    "\n",
    "In the former we are interested in finding a fixed (or minimal) subset of features with the most predictive power. In the latter we are interested in asking questions regarding specific features (or groups of features) and how they individually and in combination with other features affect prediction accuracy. Here we are only going to consider the former. The latter currently must be done manually by holding out variables, writing your own permutation code or using R. See the lecture for more details.\n",
    "\n",
    "\n",
    "### Part 7.1: Recursive feature elimination (RFE)\n",
    "In sklearn the RFE Object wraps a model. The model must produce `.feature_importances_` as part of the `.fit(...)` method. \n",
    "\n",
    "You've been given the documentation as part of practical for all other methods - this time you will need to go and look at the documentation yourself.\n",
    "\n",
    "**TASKS:**\n",
    "1. Create an RFE object (model) with and embedded Random Forest Classifier. Select 10 features.\n",
    "2. Print the generated feature importances for the selected features. BONUS: Add a list of all features to the results dictionary with zeros when the feature wasn't selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TxOur229OKtD"
   },
   "outputs": [],
   "source": [
    "# TASK 1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "selector = RFE(RandomForestClassifier(), n_features_to_select=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tGbZpqzDOKtb",
    "outputId": "300aab7e-835f-4da1-9724-675fe0e44a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Rank | RFE RF                                \n",
      "---- + --------------------------------------\n",
      "0    | kw_avg_avg                    : 0.1206\n",
      "1    | kw_max_avg                    : 0.1121\n",
      "2    | kw_avg_max                    : 0.0982\n",
      "3    | LDA_01                        : 0.0977\n",
      "4    | n_unique_tokens               : 0.0974\n",
      "5    | kw_avg_min                    : 0.0963\n",
      "6    | LDA_02                        : 0.0956\n",
      "7    | LDA_04                        : 0.0952\n",
      "8    | global_sentiment_polarity     : 0.0938\n",
      "9    | global_subjectivity           : 0.0932\n"
     ]
    }
   ],
   "source": [
    "# TASK 2\n",
    "selector.fit(X_train, y_train)\n",
    "print(selector.n_features_)\n",
    "\n",
    "\n",
    "method_name = 'RFE RF '\n",
    "feature_importance_scores = {}\n",
    "feature_importance_scores[method_name] = selector.estimator_.feature_importances_\n",
    "\n",
    "print_variable_importances( X_train.columns[selector.support_], feature_importance_scores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhEZB4s-Obwh",
    "outputId": "d3b6ef1e-4fd8-4d7e-9153-d4e93c93626f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank | RFE RF                                \n",
      "---- + --------------------------------------\n",
      "0    | kw_avg_avg                    : 0.1251\n",
      "1    | kw_max_avg                    : 0.1104\n",
      "2    | LDA_01                        : 0.0978\n",
      "3    | kw_avg_max                    : 0.0977\n",
      "4    | kw_avg_min                    : 0.0961\n",
      "5    | n_non_stop_unique_tokens      : 0.0960\n",
      "6    | global_sentiment_polarity     : 0.0954\n",
      "7    | LDA_04                        : 0.0947\n",
      "8    | LDA_02                        : 0.0943\n",
      "9    | n_tokens_content              : 0.0927\n",
      "10   | weekday_is_wednesday          : 0.0000\n",
      "11   | weekday_is_tuesday            : 0.0000\n",
      "12   | weekday_is_thursday           : 0.0000\n",
      "13   | weekday_is_sunday             : 0.0000\n",
      "14   | weekday_is_saturday           : 0.0000\n",
      "15   | weekday_is_monday             : 0.0000\n",
      "16   | weekday_is_friday             : 0.0000\n",
      "17   | title_subjectivity            : 0.0000\n",
      "18   | title_sentiment_polarity      : 0.0000\n",
      "19   | self_reference_min_shares     : 0.0000\n",
      "20   | self_reference_max_shares     : 0.0000\n",
      "21   | self_reference_avg_sharess    : 0.0000\n",
      "22   | rate_positive_words           : 0.0000\n",
      "23   | rate_negative_words           : 0.0000\n",
      "24   | num_videos                    : 0.0000\n",
      "25   | num_self_hrefs                : 0.0000\n",
      "26   | num_keywords                  : 0.0000\n",
      "27   | num_imgs                      : 0.0000\n",
      "28   | num_hrefs                     : 0.0000\n",
      "29   | n_unique_tokens               : 0.0000\n",
      "30   | n_tokens_title                : 0.0000\n",
      "31   | n_non_stop_words              : 0.0000\n",
      "32   | min_positive_polarity         : 0.0000\n",
      "33   | min_negative_polarity         : 0.0000\n",
      "34   | max_positive_polarity         : 0.0000\n",
      "35   | max_negative_polarity         : 0.0000\n",
      "36   | kw_min_min                    : 0.0000\n",
      "37   | kw_min_max                    : 0.0000\n",
      "38   | kw_min_avg                    : 0.0000\n",
      "39   | kw_max_min                    : 0.0000\n",
      "40   | kw_max_max                    : 0.0000\n",
      "41   | is_weekend                    : 0.0000\n",
      "42   | global_subjectivity           : 0.0000\n",
      "43   | global_rate_positive_words    : 0.0000\n",
      "44   | global_rate_negative_words    : 0.0000\n",
      "45   | data_channel_is_world         : 0.0000\n",
      "46   | data_channel_is_tech          : 0.0000\n",
      "47   | data_channel_is_socmed        : 0.0000\n",
      "48   | data_channel_is_lifestyle     : 0.0000\n",
      "49   | data_channel_is_entertainment : 0.0000\n",
      "50   | data_channel_is_bus           : 0.0000\n",
      "51   | avg_positive_polarity         : 0.0000\n",
      "52   | avg_negative_polarity         : 0.0000\n",
      "53   | average_token_length          : 0.0000\n",
      "54   | abs_title_subjectivity        : 0.0000\n",
      "55   | abs_title_sentiment_polarity  : 0.0000\n",
      "56   | LDA_03                        : 0.0000\n",
      "57   | LDA_00                        : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# BONUS\n",
    "\n",
    "size = X_train.columns[np.invert(selector.support_)].shape\n",
    "zeros = np.zeros(shape=size)\n",
    "feature_scores = {}\n",
    "feature_scores['RFE RF'] = np.append(selector.estimator_.feature_importances_, zeros)\n",
    "columns = np.append(X_train.columns[selector.support_], X_train.columns[np.invert(selector.support_)])\n",
    "print_variable_importances( columns, feature_scores, show_top=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5WE2JTB2Mij"
   },
   "source": [
    "### Part 7.2: All relevant feature selection\n",
    "Instead of asking for a fixed number of features, we may simply want to throw away those that are not relevant. This is a more advanced method to **removing features with no/low variance**. I.e. we are looking to throw away features we think do not have any **real** impact. By real impact we mean no significant impact in a generalized sense. While we could have simply defined a threshold for RFE (stopping throwing away features once the permutation importance, i.e. change in mean prediction accuracy, was not zero) we would expect some change in prediction accuracy due to random chance. Addressing this issue the Boruta package finds iteratively throws aways the worst feature of the set, stopping when the change in mean prediction accuracy is greater than chance.\n",
    "\n",
    "**Note: The approach is still a heuristic and may throw away valuable features since it removes features in a greedy fashion. In addition throwing away features now that look like they have no affect may be premature, as your training set grows they may be important.**\n",
    "\n",
    "This is done by wrapping a model (again, one whose `.fit(..)` method produces `.feature_importances_`).\n",
    "\n",
    "**TASKS:**\n",
    "1. Fit a Boruta Object (model) by wrapping a RandomForestClassifier.\n",
    "2. Print the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcBqFr5g4kcX",
    "outputId": "c013cf3e-a4ad-442f-f247-89d073b0dbfc"
   },
   "outputs": [],
   "source": [
    "!pip install -q Boruta # install the package. If this doesn't work run in a terminal: sudo pip3 install Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "tMbN4o6A4s6T"
   },
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "selector = BorutaPy(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Yi3-UNIObwk"
   },
   "outputs": [],
   "source": [
    "selector.fit(X.values, y.values)\n",
    "features = selector.transform(X.values)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 4 solution 2022",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
