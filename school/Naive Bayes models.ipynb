{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Analytics Specializations & Applications - Week 4\n",
    "\n",
    "# Text Analytics 2 - Case Study Exercises\n",
    "----------\n",
    "\n",
    "This set of exercises assumes that you have completed the accompanying **\"Text Analytics 1 - Preparatory Exercises\"** jupyter notebook. If you haven't, please find and run through that set of exercises first.\n",
    "\n",
    "### Scenario\n",
    "Now we have the tools we need under our belt, consider the following case study scenario. We are a communications/media company like Krow Communications, whose outputs are now receiving reviews and discussion on the web (particularly in the form of youtube comments). We would to generate a text analytics solution that will take these reviews (which we assume to be unstructured text alone), perfrom some text analytics on them, and then tell us if that review was positive or negative (to do this we will need to perform sentiment analysis). Once we have a method of doing this we can score the success of our outputs in an automatic fashion (and also potentially gauge how reaction towards them changes over time). \n",
    "\n",
    "The problem is that we currently have no basis for assessing reviews - our media outputs don't get \"scored\". This problem is called the \"cold start\" problem - we just don't have any ground truth which we can build a text analytics model against. \n",
    "\n",
    "Luckily, we may be able to leverage some \"transfer learning\" - one of our partners has a dataset of movie reviews that **are** accompanied with a score - so we know if the text they include is broadly positive or negative.\n",
    "\n",
    "By performing text analytics on this dataset and concentrating on sentiment (rather than movie actors, directors, genres, etc) we will be able to create a natural lanugage model that will receive any review - such as those we get discussing our advertising campaigns - and tell us something about the author's reaction. This we can then document, and use in our future pitches.\n",
    "\n",
    "### The dataset\n",
    "Our transfer dataset consists of 25,000 written movie reviews from the Internet Movie Database, IMDb (www.imdb.com). No movie has more than 30 reviews, and the review text is accompanied by a binary score (with the value 1 if the manual IMDb rating for that review is greater than 6, and the value 0 if the rating is less than 5). From this data we will learn what constitutes a positive and negative review in terms of text).\n",
    "\n",
    "To analyse this text, so we can understand what consitutes a positive and negative reivew in terms of language, we will implement the following:\n",
    "\n",
    "* Data Collation\n",
    "* Stripping / Case Folding\n",
    "* Stemming\n",
    "* Stopping\n",
    "* Tokenization \n",
    "* Vectorization (and TF-IDF)\n",
    "* Testing (using Cosine Similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading in the data (which is provided in a file in the same folder as this exercise)...\n",
    "\n",
    "\n",
    "<span style=\"font-weight:bold; color:green;\">&rarr; Load in and examine the first ten lines of the data <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good, but come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend's house. I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for £1, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;This movie is full of references. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentiment                                             review\n",
       "0   5814_8          1  With all this stuff going down at the moment w...\n",
       "1   2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2   7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3   3630_4          0  It must be assumed that those who praised this...\n",
       "4   9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "5   8196_8          1  I dont know why people think this is such a ba...\n",
       "6   7166_2          0  This movie could have been very good, but come...\n",
       "7  10633_1          0  I watched this video at a friend's house. I'm ...\n",
       "8    319_1          0  A friend of mine bought this film for £1, and ...\n",
       "9  8713_10          1  <br /><br />This movie is full of references. ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv(\"movie_data.tsv\", delimiter=\"\\t\")\n",
    "\n",
    "#-- examine the first 10 lines of the data here\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the last entry - see how it has html tags in it. We need to get rid of these (and let's lose the punctuation while we are at it), so let's first do some stripping. I've created a custom function to do this which is out of scope of this course, so for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good, but come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend's house. I'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for £1, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie is full of references. Like \\Mad Ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentiment                                             review\n",
       "0   5814_8          1  With all this stuff going down at the moment w...\n",
       "1   2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2   7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3   3630_4          0  It must be assumed that those who praised this...\n",
       "4   9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "5   8196_8          1  I dont know why people think this is such a ba...\n",
       "6   7166_2          0  This movie could have been very good, but come...\n",
       "7  10633_1          0  I watched this video at a friend's house. I'm ...\n",
       "8    319_1          0  A friend of mine bought this film for £1, and ...\n",
       "9  8713_10          1  This movie is full of references. Like \\Mad Ma..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html_cleaner\n",
    "data.review = html_cleaner.remove_html(data.review)\n",
    "\n",
    "#-- examine the first 10 lines of the data again\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.50001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  25000.00000\n",
       "mean       0.50000\n",
       "std        0.50001\n",
       "min        0.00000\n",
       "25%        0.00000\n",
       "50%        0.50000\n",
       "75%        1.00000\n",
       "max        1.00000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- describe the data here\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to learn a model that can recognize those reviews with positive sentiment form those with negative sentiment, so fist split the data into test and training sets (use the first 20,000 items for the training data nd the rest for the test data.\n",
    "\n",
    "<span style=\"font-weight:bold; color:green;\">&rarr; Split the data into test and training <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\"], axis = 1)\n",
    "train_data = data[:20000]\n",
    "test_data = data[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, as before the next step is to vectorize our text data - let's do that first with a simple Count Vectorizer (and examine how much TF-IDF can improve things later)'\n",
    "\n",
    "<span style=\"font-weight:bold; color:green;\">&rarr; Complete the following code <span/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test data successfully vectorized\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#-- create our vectorizer object, ready to fit and \n",
    "#-- transform our data into a vector space format\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#-- setup the model's feature space using our training data\n",
    "vectorizer.fit(train_data.review)\n",
    "\n",
    "#-- and then convert the training data set into vector format\n",
    "train_features = vectorizer.transform(train_data.review)\n",
    "\n",
    "#-- while we are here convert our test dataset in the same way\n",
    "test_features = vectorizer.transform(test_data.review)\n",
    "\n",
    "print(\"Training and test data successfully vectorized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a model that will understand how sentiment is constructued out of text in some way. For this job we could use any classifier, but given Naive Bayes models have historically been used in text analysis, let's maintain that tradition here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic Model successfuly created\n"
     ]
    }
   ],
   "source": [
    "#-- let's use a multinomial naive bayes classifer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = MultinomialNB()\n",
    "\n",
    "#-- fit the model to our training data - note in this step the model is\n",
    "#-- finding the relationship between word frequencies and the sentiment\n",
    "#-- of each review\n",
    "NB.fit(train_features, train_data.sentiment)\n",
    "print(\"Linguistic Model successfuly created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well our model works, by testing it on our holdout dataset (note that we would normally cross-validate here to get a more representative score, but a single holdout test is fine for now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.28052187e-01 7.19478130e-02]\n",
      " [9.89831089e-01 1.01689113e-02]\n",
      " [3.18273809e-02 9.68172619e-01]\n",
      " ...\n",
      " [9.99454524e-01 5.45475684e-04]\n",
      " [9.91074863e-01 8.92513651e-03]\n",
      " [8.89195479e-05 9.99911080e-01]]\n"
     ]
    }
   ],
   "source": [
    "#-- generate some predictions\n",
    "results = NB.predict_proba(test_features)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results come in two columns for each review - the first column is the probability that it is a negative review, and the second if it is a positive review. We can come up with an actual prediction of whether the review contains positive sentiment or not by seeing if the second column is > 0.5 or not (our threshold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True ... False False  True]\n"
     ]
    }
   ],
   "source": [
    "#-- note the neat syntax here: First we index the result's second column using\n",
    "#-- [:,1] and then we test if it is more than 0.5 and hence a positive review\n",
    "predictions = results[:,1] > 0.5\n",
    "\n",
    "#-- the columns which were more than 0.5 are designated as True\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predicted the sentiment of 84.2% of reviews correctly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(test_data.sentiment, predictions)\n",
    "print(\"We predicted the sentiment of {0:.01f}% of reviews correctly\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% is not bad at all, given we are using a simple and quick bag of words approach. In fact this is no doubt good enough for the business task, and we could start applying the model to our own reviews.\n",
    "\n",
    "However, we can do better as we've omitted some useful steps. Your challenge is now to wee how much you can improve the results this model by implementing:\n",
    "> * Stopping\n",
    "> * Stemming\n",
    "> * Case Folding\n",
    "> * and a TfifdVectorizer() \n",
    "\n",
    "Also consider:\n",
    "> * What can you find out about the important features (i.e. which words are most influential?) \n",
    "> * Can you design a query that fools the model? - tip. consider including negative words even though the review is good...\n",
    "\n",
    "Good luck! And ask for help if you run out of ideas - we will be going through the solution on Tuesday, along with the coursework release.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "### SOLUTIONS SECTION\n",
    "\n",
    "Below is an example solution to some of the above challenges. These aren't the only ways of doing all the data cleansing, and finding important features, so if you've found another way then that's great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is code that creates a vectorizer that uses TFIDF, which we give a tokenizer that uses only alphabetical words. Note that doing the vectorization can take a few minutes to run (the star next to the code block in jupyter indicates it is still processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved text vectorizer successfully created...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavinshi/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training reviews successfully vectorized...\n",
      "Testing reviews successfully vectorized.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import html_cleaner\n",
    "\n",
    "#-- first let's create a function that will do our stemming\n",
    "#-- for us when we need it to, along with removing punctuation...\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = []\n",
    "    \n",
    "    #-- let's just extract alphabetical words of at least 3 characters\n",
    "    tokens = nltk.RegexpTokenizer(r'[a-zA-Z]{3,}').tokenize(text)\n",
    "    for t in tokens:\n",
    "        stemmed_tokens.append(stemmer.stem(t))\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "#-- create our vectorizer object, and fit our data, cleansed by:\n",
    "#-- 1. supplying the vectorizer our customer stemming functino\n",
    "#-- 2. telling it to use it's inbuilt stop words\n",
    "#-- 3. telling it to convert all the text to lowercase itself\n",
    "vectorizer_new = TfidfVectorizer(\n",
    "                 tokenizer = tokenize_and_stem,\n",
    "                 stop_words='english', \n",
    "                 lowercase=True)\n",
    "print(\"Improved text vectorizer successfully created...\")\n",
    "\n",
    "#-- and use this upgraded vectorizer to fit our customer reviews\n",
    "train_features = vectorizer_new.fit_transform(train_data.review)\n",
    "print(\"Training reviews successfully vectorized...\")\n",
    "\n",
    "test_features = vectorizer_new.transform(test_data.review)\n",
    "print(\"Testing reviews successfully vectorized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linguistic Model successfuly created\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB_new = MultinomialNB()\n",
    "NB_new.fit(train_features, train_data.sentiment)\n",
    "print(\"Linguistic Model successfuly created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We predicted the sentiment of 85.50% of reviews correctly\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#-- lets run our test data through the sentiment analyser\n",
    "results = NB_new.predict_proba(test_features)\n",
    "\n",
    "#-- and then see how good its predictions were now\n",
    "predictions = results[:,1] > 0.5\n",
    "acc = accuracy_score(test_data.sentiment, predictions)\n",
    "print(\"We predicted the sentiment of {0:.2f}% of reviews correctly\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well 85.5% is definitely an improvement (even if 1.3% doesn't seem a huge one in this case, it could well be significant in practice!). Now let's move on to see if we can unpick how our customer review analyser is working and address the question:\n",
    "\n",
    "> * What can you find out about the important features (i.e. which words are most influential?) \n",
    "\n",
    "Our text classification model gives every word a coefficient to show how important it is - the higher the better (they are negative so this translates to the closer to zero the better). Let's first join these up with their corresponding names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-11.709844112606802, 'brenna'), (-10.408091352572558, 'brennan'), (-11.509167275048224, 'brennen'), (-10.415827639374001, 'brent'), (-11.6360779618455, 'brenten'), (-11.528025678409094, 'brentwood'), (-11.709844112606802, 'brereton'), (-11.709844112606802, 'breslin'), (-11.709844112606802, 'bresnahan'), (-10.876968869350712, 'bressart')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavinshi/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#-- extract the feature names\n",
    "feature_names = vectorizer_new.get_feature_names()\n",
    "\n",
    "#-- extract the coefficient assigned to each feature\n",
    "coefficients = NB_new.coef_[0]\n",
    "\n",
    "#-- pair the words and their coefficients\n",
    "word_coefficients = list(zip(coefficients, feature_names))\n",
    "print( word_coefficients[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the important words that indicate sentiment, we now just need to sort these words in order of their coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- sort the word based on their coefficient score - note the odd \"lambda\"\n",
    "#-- syntax in the function - this essentially just says use the first \n",
    "#-- element (i.e th score) of the coefficient/name pairs to do the sorting:\n",
    "word_importance = sorted(word_coefficients, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an ordered list, we can look at the last 20 say, to find the words that are important in detecting sentiment - and hence distinguishing positive reviews from negative ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.711050926237108 play\n",
      "-6.666375677565675 best\n",
      "-6.579716975004585 realli\n",
      "-6.575640906766945 make\n",
      "-6.522937570646216 just\n",
      "-6.472685608123468 charact\n",
      "-6.401405032632604 watch\n",
      "-6.361756448785925 love\n",
      "-6.361305580799283 time\n",
      "-6.360537299145098 ha\n",
      "-6.3553334816843305 stori\n",
      "-6.335025571568903 good\n",
      "-6.284764713363029 great\n",
      "-6.262635951894112 veri\n",
      "-6.237542071214977 like\n",
      "-5.92191010047866 hi\n",
      "-5.6724097783330265 wa\n",
      "-5.5137134989245595 film\n",
      "-5.500440529092075 movi\n",
      "-5.401471479131113 thi\n"
     ]
    }
   ],
   "source": [
    "for a,b in word_importance[-20:]:\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it has some sense - important words like \"like\", \"good\", \"great\" and \"love\" are high up there as we'd expect. But also words like \"story\" and \"characters\", which are coherent with what people are looking for in good movies. \n",
    "\n",
    "In terms of \"transfer learning\" these items, however, may not apply to adverts/brands - what still remains is to try and create a review that can fool our model to explore this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure about this advert - it is selling a bad brand! --> NEGATIVE REVIEW\n",
      "I love this advert - it is selling a good brand! --> POSITIVE REVIEW\n",
      "This is excellent work --> POSITIVE REVIEW\n",
      "What is this rubbish? --> NEGATIVE REVIEW\n",
      "Please save us from this nonsense --> NEGATIVE REVIEW\n",
      "I enjoyed watching this --> POSITIVE REVIEW\n",
      "I wanted to say this advert is bad, but I can't - just the opposite in fact! --> NEGATIVE REVIEW\n"
     ]
    }
   ],
   "source": [
    "#-- create some test \"reviews\"\n",
    "test_reviews = [\n",
    "    \"I'm not sure about this advert - it is selling a bad brand!\",\n",
    "    \"I love this advert - it is selling a good brand!\",\n",
    "    \"This is excellent work\",\n",
    "    \"What is this rubbish?\",\n",
    "    \"Please save us from this nonsense\",\n",
    "    \"I enjoyed watching this\",\n",
    "    \"I wanted to say this advert is bad, but I can't - just the opposite in fact!\"\n",
    "]\n",
    "\n",
    "#-- vectorize it\n",
    "vec_test = vectorizer_new.transform( test_reviews )\n",
    "\n",
    "#-- Run it through the sentiment analyser\n",
    "results = NB_new.predict_proba(vec_test)\n",
    "\n",
    "#-- Examine the sentiment the model detects\n",
    "for t, r in zip(test_reviews, results):\n",
    "    if r[1] > 0.5:\n",
    "        print(t, \"--> POSITIVE REVIEW\")\n",
    "    else:\n",
    "        print(t, \"--> NEGATIVE REVIEW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how all the reviews are categorized correctly... apart from the last one. With some carefully worded phrasing, we have tricked our model. Nonetheless even with a simple bag of words approach we have a useful tool for the business, which we can now use to track the companies influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
